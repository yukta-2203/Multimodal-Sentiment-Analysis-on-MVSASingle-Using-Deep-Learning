{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2m1E0jed0Cyz"
      },
      "outputs": [],
      "source": [
        "!pip install -q torch torchvision transformers pillow pandas scikit-learn"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "ZIP_PATH = \"/content/mvsa.zip\"   # change if filename is different\n",
        "\n",
        "!mkdir -p /content/mvsa_raw\n",
        "!unzip -o \"$ZIP_PATH\" -d /content/mvsa_raw\n",
        "\n",
        "!ls -R /content/mvsa_raw"
      ],
      "metadata": {
        "id": "7OJAuKSN1XN2",
        "collapsed": true
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "from pathlib import Path\n",
        "import pandas as pd\n",
        "\n",
        "ROOT_DIR   = \"/content/mvsa_raw/MVSA_Single\"\n",
        "DATA_DIR   = os.path.join(ROOT_DIR, \"data\")\n",
        "LABEL_FILE = os.path.join(ROOT_DIR, \"labelResultAll.txt\")\n",
        "\n",
        "print(\"ROOT_DIR :\", ROOT_DIR)\n",
        "print(\"DATA_DIR :\", DATA_DIR)\n",
        "print(\"LABEL_FILE:\", LABEL_FILE)\n",
        "\n",
        "# 1) Read labelResultAll.txt\n",
        "# First column: ID\n",
        "# Second column header: \"text,image\"\n",
        "labels_raw = pd.read_csv(LABEL_FILE, sep=r\"\\s+\", engine=\"python\")\n",
        "print(labels_raw.head())\n",
        "print(\"Columns:\", labels_raw.columns)\n",
        "\n",
        "# 2) Split \"text,image\" into two sentiment columns\n",
        "sent_col = labels_raw.columns[1]  # should be \"text,image\"\n",
        "labels_raw[[\"text_sent\", \"image_sent\"]] = labels_raw[sent_col].str.split(\",\", expand=True)\n",
        "\n",
        "# 3) Use text sentiment as label\n",
        "labels = labels_raw[[\"ID\", \"text_sent\"]].copy()\n",
        "labels[\"ID\"] = labels[\"ID\"].astype(int)\n",
        "labels[\"text_sent\"] = labels[\"text_sent\"].str.lower().str.strip()\n",
        "\n",
        "sent_to_id = {\"negative\": 0, \"neutral\": 1, \"positive\": 2}\n",
        "labels[\"label\"] = labels[\"text_sent\"].map(sent_to_id)\n",
        "\n",
        "print(\"Sentiment mapping:\", sent_to_id)\n",
        "print(labels.head())"
      ],
      "metadata": {
        "id": "Qln67CsE1tXL"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "rows = []\n",
        "\n",
        "for _, row in labels.iterrows():\n",
        "    id_int = row[\"ID\"]\n",
        "    id_str = str(id_int)\n",
        "\n",
        "    img_path = os.path.join(DATA_DIR, f\"{id_str}.jpg\")\n",
        "    txt_path = os.path.join(DATA_DIR, f\"{id_str}.txt\")\n",
        "\n",
        "    # ensure both files exist\n",
        "    if not (os.path.exists(img_path) and os.path.exists(txt_path)):\n",
        "        continue\n",
        "\n",
        "    # read text\n",
        "    with open(txt_path, \"r\", encoding=\"utf-8\", errors=\"ignore\") as f:\n",
        "        text = f.read().strip()\n",
        "\n",
        "    if not text:\n",
        "        continue\n",
        "\n",
        "    rows.append({\n",
        "        \"id\": id_int,\n",
        "        \"text\": text,\n",
        "        \"image_path\": img_path,\n",
        "        \"label\": row[\"label\"],\n",
        "    })\n",
        "\n",
        "df = pd.DataFrame(rows)\n",
        "print(df.head())\n",
        "print(\"Total usable samples:\", len(df))\n",
        "\n",
        "# Optional: subsample for quicker training\n",
        "N_SAMPLES = 4000  # adjust as you like\n",
        "if len(df) > N_SAMPLES:\n",
        "    df = df.sample(N_SAMPLES, random_state=42).reset_index(drop=True)\n",
        "\n",
        "print(\"Using samples:\", len(df))\n",
        "df[\"label\"].value_counts()"
      ],
      "metadata": {
        "id": "jEIkGjDu1wEA"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.model_selection import train_test_split\n",
        "from transformers import AutoTokenizer\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from PIL import Image\n",
        "import torch\n",
        "\n",
        "tokenizer = AutoTokenizer.from_pretrained(\"bert-base-uncased\")\n",
        "MAX_LEN = 64\n",
        "\n",
        "# Image transforms\n",
        "train_transform = transforms.Compose([\n",
        "    transforms.Resize((256, 256)),\n",
        "    transforms.RandomResizedCrop(224, scale=(0.8, 1.0)),\n",
        "    transforms.RandomHorizontalFlip(),\n",
        "    transforms.ColorJitter(brightness=0.1, contrast=0.1,\n",
        "                           saturation=0.1, hue=0.02),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "val_transform = transforms.Compose([\n",
        "    transforms.Resize((224, 224)),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485,0.456,0.406],[0.229,0.224,0.225]),\n",
        "])\n",
        "\n",
        "class MVSASingleDataset(Dataset):\n",
        "    def __init__(self, df, transform):\n",
        "        self.df = df.reset_index(drop=True)\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.df)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        row = self.df.iloc[idx]\n",
        "\n",
        "        # text â†’ tokens\n",
        "        text = str(row[\"text\"])\n",
        "        enc = tokenizer(\n",
        "            text,\n",
        "            padding=\"max_length\",\n",
        "            truncation=True,\n",
        "            max_length=MAX_LEN,\n",
        "            return_tensors=\"pt\"\n",
        "        )\n",
        "\n",
        "        # image\n",
        "        image = Image.open(row[\"image_path\"]).convert(\"RGB\")\n",
        "        image = self.transform(image)\n",
        "\n",
        "        label = torch.tensor(row[\"label\"], dtype=torch.long)\n",
        "\n",
        "        return {\n",
        "            \"input_ids\": enc[\"input_ids\"].squeeze(0),\n",
        "            \"attention_mask\": enc[\"attention_mask\"].squeeze(0),\n",
        "            \"image\": image,\n",
        "            \"label\": label\n",
        "        }\n",
        "\n",
        "# Stratified split\n",
        "train_df, val_df = train_test_split(\n",
        "    df,\n",
        "    test_size=0.2,\n",
        "    stratify=df[\"label\"],\n",
        "    random_state=42\n",
        ")\n",
        "\n",
        "train_dataset = MVSASingleDataset(train_df, transform=train_transform)\n",
        "val_dataset   = MVSASingleDataset(val_df,   transform=val_transform)\n",
        "\n",
        "BATCH_SIZE = 16\n",
        "\n",
        "train_loader = DataLoader(train_dataset, batch_size=BATCH_SIZE, shuffle=True)\n",
        "val_loader   = DataLoader(val_dataset,   batch_size=BATCH_SIZE, shuffle=False)\n",
        "\n",
        "len(train_loader), len(val_loader)"
      ],
      "metadata": {
        "id": "r1jkskSR1yoM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from transformers import AutoModel\n",
        "from torchvision import models\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "print(\"Device:\", device)\n",
        "\n",
        "class MultimodalSentimentNet(nn.Module):\n",
        "    def __init__(self, num_classes=3):\n",
        "        super().__init__()\n",
        "\n",
        "        # Text encoder\n",
        "        self.text_encoder = AutoModel.from_pretrained(\"bert-base-uncased\")\n",
        "        text_dim = self.text_encoder.config.hidden_size  # 768\n",
        "\n",
        "        # Image encoder\n",
        "        resnet = models.resnet18(weights=models.ResNet18_Weights.DEFAULT)\n",
        "        img_dim = resnet.fc.in_features                  # 512\n",
        "        resnet.fc = nn.Identity()\n",
        "        self.image_encoder = resnet\n",
        "\n",
        "        # Fusion classifier\n",
        "        fusion_dim = text_dim + img_dim  # 768 + 512\n",
        "        self.classifier = nn.Sequential(\n",
        "            nn.Linear(fusion_dim, 256),\n",
        "            nn.ReLU(),\n",
        "            nn.Dropout(0.4),\n",
        "            nn.Linear(256, num_classes)\n",
        "        )\n",
        "\n",
        "    def forward(self, input_ids, attention_mask, images):\n",
        "        # Text branch\n",
        "        text_out = self.text_encoder(\n",
        "            input_ids=input_ids,\n",
        "            attention_mask=attention_mask\n",
        "        )\n",
        "        text_emb = text_out.last_hidden_state[:, 0, :]  # CLS token\n",
        "\n",
        "        # Image branch\n",
        "        img_emb = self.image_encoder(images)\n",
        "\n",
        "        # Fuse\n",
        "        fused = torch.cat([text_emb, img_emb], dim=1)\n",
        "        logits = self.classifier(fused)\n",
        "        return logits\n",
        "\n",
        "NUM_CLASSES = 3  # negative / neutral / positive\n",
        "model = MultimodalSentimentNet(num_classes=NUM_CLASSES).to(device)"
      ],
      "metadata": {
        "id": "LFL0N6OB11m0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import torch.nn as nn\n",
        "from torch.optim import AdamW\n",
        "\n",
        "# Class weights (handle imbalance)\n",
        "class_counts = train_df[\"label\"].value_counts().sort_index().values.astype(np.float32)\n",
        "inv = 1.0 / class_counts\n",
        "weights = inv / inv.sum() * len(class_counts)\n",
        "\n",
        "class_weights = torch.tensor(weights, dtype=torch.float32).to(device)\n",
        "print(\"Class counts :\", class_counts)\n",
        "print(\"Class weights:\", class_weights)\n",
        "\n",
        "criterion = nn.CrossEntropyLoss(weight=class_weights)\n",
        "optimizer = AdamW(model.parameters(), lr=1e-4, weight_decay=1e-2)"
      ],
      "metadata": {
        "id": "m-_JsIJx14JQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from torch.nn.functional import cross_entropy\n",
        "\n",
        "def run_epoch(loader, train=True):\n",
        "    if train:\n",
        "        model.train()\n",
        "    else:\n",
        "        model.eval()\n",
        "\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "\n",
        "    for batch in loader:\n",
        "        input_ids = batch[\"input_ids\"].to(device)\n",
        "        attention_mask = batch[\"attention_mask\"].to(device)\n",
        "        images = batch[\"image\"].to(device)\n",
        "        labels = batch[\"label\"].to(device)\n",
        "\n",
        "        if train:\n",
        "            optimizer.zero_grad()\n",
        "\n",
        "        with torch.set_grad_enabled(train):\n",
        "            logits = model(input_ids, attention_mask, images)\n",
        "            loss = criterion(logits, labels)\n",
        "\n",
        "            if train:\n",
        "                loss.backward()\n",
        "                torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=1.0)\n",
        "                optimizer.step()\n",
        "\n",
        "        preds = torch.argmax(logits, dim=1)\n",
        "        correct += (preds == labels).sum().item()\n",
        "        total += labels.size(0)\n",
        "        total_loss += loss.item()\n",
        "\n",
        "    avg_loss = total_loss / len(loader)\n",
        "    acc = correct / total\n",
        "    return avg_loss, acc\n",
        "\n",
        "\n",
        "EPOCHS = 15\n",
        "best_val_acc = 0.0\n",
        "patience = 3\n",
        "patience_counter = 0\n",
        "best_path = \"/content/mvsa_multimodal_best.pt\"\n",
        "\n",
        "for epoch in range(1, EPOCHS + 1):\n",
        "    train_loss, train_acc = run_epoch(train_loader, train=True)\n",
        "    val_loss, val_acc = run_epoch(val_loader, train=False)\n",
        "\n",
        "    print(f\"Epoch {epoch:02d}: \"\n",
        "          f\"train_loss={train_loss:.4f}, train_acc={train_acc:.3f}, \"\n",
        "          f\"val_loss={val_loss:.4f}, val_acc={val_acc:.3f}\")\n",
        "\n",
        "    # Early stopping on val_acc\n",
        "    if val_acc > best_val_acc:\n",
        "        best_val_acc = val_acc\n",
        "        patience_counter = 0\n",
        "        torch.save(model.state_dict(), best_path)\n",
        "        print(f\"  ðŸ”¸ New best model saved (val_acc={best_val_acc:.3f})\")\n",
        "    else:\n",
        "        patience_counter += 1\n",
        "        if patience_counter >= patience:\n",
        "            print(\"  ðŸ”» Early stopping triggered.\")\n",
        "            break\n",
        "\n",
        "print(\"Best validation accuracy:\", best_val_acc)"
      ],
      "metadata": {
        "id": "PcqEgJ7016n3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model.load_state_dict(torch.load(best_path, map_location=device))\n",
        "model.eval()\n",
        "\n",
        "FINAL_SAVE = \"/content/mvsa_multimodal_sentiment.pt\"\n",
        "torch.save(model.state_dict(), FINAL_SAVE)\n",
        "print(\"Final best model saved at:\", FINAL_SAVE)"
      ],
      "metadata": {
        "id": "LoOTPiV618dp"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}